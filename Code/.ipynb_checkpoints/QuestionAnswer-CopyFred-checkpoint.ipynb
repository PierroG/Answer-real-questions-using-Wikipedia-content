{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "import truecase\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ROOT\n",
      "what det\n",
      "direction dobj\n",
      "does aux\n",
      "the det\n",
      "sun nsubj\n",
      "rise pcomp\n",
      "in prep\n",
      "the det\n",
      "morning pobj\n",
      "? punct\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp('From what direction does the sun rise in the morning ?')\n",
    "for token in doc : \n",
    "    print(token.text, token.dep_)\n",
    "#for token in doc: \n",
    "    #print(token.text)\n",
    "    #print(token.dep_)\n",
    "    #print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Model function\n",
    "Use loaded BERT model to return answer<br>\n",
    "**Input** : Question, Context text <br>\n",
    "**return** : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    print(\"I'm looking for an answer, please wait ...\")\n",
    "    # == Tokenize ==\n",
    "    # use a python dictonary so run on CPU\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    print(\"-Tokenization\")\n",
    "    input_ids = tokenizer.encode(question, answer_text, add_special_tokens=True) #special_token -> correction bug no 102 token\n",
    "    #print(\"input ids : \", input_ids)\n",
    "    #print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens including the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    # by default on CPU, use model.to(device) to select GPU !?\n",
    "    print(\"-Forward pass on the model\")\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from \n",
    "\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    #print(type(start_scores))\n",
    "    #print(start_scores.size())\n",
    "    #print(start_scores[0,answer_start])\n",
    "    #print(end_scores[0,answer_end])\n",
    "    \n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Question Processing\n",
    "Extract subjet _(and more?)_ from the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ancient greek]\n",
      "['nsubj']\n",
      "subject found by noun: ancient greek \n",
      "\n",
      "['Barack Obama']\n",
      "['nsubj']\n",
      "subject found by ent :  PERSON Barack Obama \n",
      "\n",
      "['ROMEO AND JULIET']\n",
      "['attr', 'nsubj', 'pobj', 'nsubj', 'dobj', 'conj']\n",
      "subject found by ent :  WORK_OF_ART ROMEO AND JULIET \n",
      "\n",
      "[the size, the moon]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: the moon \n",
      "\n",
      "['The Lord of The Rings']\n",
      "['nsubj', 'dobj', 'pobj']\n",
      "subject found by ent :  WORK_OF_ART The Lord of The Rings \n",
      "\n",
      "[]\n",
      "['attr']\n",
      "subject not found, please try another formulation \n",
      "\n",
      "[the sky]\n",
      "['attr', 'nsubj']\n",
      "subject found by noun: the sky \n",
      "\n",
      "[the color, the sky]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: the sky \n",
      "\n",
      "[the meaning, bread]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: bread \n",
      "\n",
      "[the meaning]\n",
      "['attr', 'nsubj']\n",
      "subject found by noun: the meaning \n",
      "\n",
      "[the meaning, OMG]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: OMG \n",
      "\n",
      "[the meaning]\n",
      "['attr', 'nsubj']\n",
      "subject found by noun: the meaning \n",
      "\n",
      "[the third color, the french flag]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: the french flag \n",
      "\n",
      "['Nirvana']\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by ent :  PERSON Nirvana \n",
      "\n",
      "[hops, the brewing process]\n",
      "['nsubjpass', 'pobj']\n",
      "subject found by noun: the brewing process \n",
      "\n",
      "[what direction, the sun, the morning]\n",
      "['dobj', 'nsubj', 'pobj']\n",
      "subject found by noun: the morning \n",
      "\n",
      "['Nirvana']\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by ent :  PERSON Nirvana \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject': 'Nirvana', 'infos': [the height, Nirvana's singer]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_subject_with_spacy(question):\n",
    "    \n",
    "    #question = truecase.get_true_case(question) #le truecaser est un peu bidon j'ai l'impression\n",
    "    #print(question)\n",
    "    \n",
    "    subject_dict = {'subject' : '', 'infos' : []} #dictionnaire qui contiendra le sujet, et les infos complémentaires\n",
    "    \n",
    "    osef_list = ['who','why','what','when','which','how', 'Who','Why','What','When','Which', 'How'] #noun to not take into account\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    #on prépare une liste des noms communs (ou plus précisent chunks, qui peuvent être des groupes nominaux plus larges, des unités de sens) de la question\n",
    "    \n",
    "    nouns_list = []\n",
    "    dep_list = []\n",
    "    for noun in doc.noun_chunks :\n",
    "        dep_list.append(noun.root.dep_)\n",
    "        nouns_list.append(noun)\n",
    "           \n",
    "    #on enlève de la liste des potentiels sujets les mots interrogatifs venant de osef_list\n",
    "    for noun in nouns_list : \n",
    "        if str(noun) in osef_list : \n",
    "            nouns_list.remove(noun)\n",
    "                \n",
    "    #on crée une liste d'entité nommées de la phrase. S'il y en a une dans la question, alors c'est le sujet\n",
    "    #nn crée également une liste qui va contenir les labels de ces entités nommées, car certains types d'entités ne nous intéressent pas\n",
    "    #les labels qui nous intéressent sont dans la liste relevant_labels\n",
    "    \n",
    "    #si il y a des entités nommées, on les utilise comme sujet et les chunks alentours comme infos supplémentaires\n",
    "    #si il n'y a pas d'entités nommées, on va uniquement regarder les chunks (dans le 'else')\n",
    "    \n",
    "    ents_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    relevant_labels = ['PERSON','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW']\n",
    "    for ent in doc.ents :\n",
    "        if ent.label_ in relevant_labels : \n",
    "            ents_list.append(ent.text)\n",
    "            labels_list.append(ent.label_)\n",
    "            #dep_list.append(ent.dep_) #pour tests sur depencies\n",
    "            \n",
    "    if ents_list and labels_list : \n",
    "        print(ents_list)\n",
    "        print(dep_list)\n",
    "        print('subject found by ent : ', labels_list[-1] , ents_list[-1], '\\n')\n",
    "        subject_dict['subject'] = ents_list[-1] #on renvoie la dernière entité nommée pertinente trouvée\n",
    "        for other_noun in nouns_list : \n",
    "            subject_dict['infos'].append(other_noun)\n",
    "        return(subject_dict)\n",
    "            \n",
    "    \n",
    "    else : \n",
    "            \n",
    "    #si notre liste de chunks potentiels sujets est vide : pas de sujet\n",
    "    #si elle est égal à 1 : pas de doute, le sujet est cet élément\n",
    "    #si elle est plus grande que 1, le sujet est le deuxième élément\n",
    "    #règle simpliste mais qui semble suivre la logique de la formulation d'une question : c'est souvent le second nom qui est le sujet dans les questions qui en comportent deux, j'ai l'impression \n",
    "        \n",
    "        print(nouns_list)\n",
    "        print(dep_list)\n",
    "        if(len(nouns_list)) == 0 :\n",
    "            print(\"subject not found, please try another formulation\", '\\n')\n",
    "        else :\n",
    "            print(\"subject found by noun: \" + str(nouns_list[-1]), '\\n')\n",
    "            subject_dict['subject'] = str(nouns_list[-1]) #le sujet est le dernier chunk\n",
    "            for other_noun in nouns_list[0:-1] : #dans ces cas de figure avec + d'un nom, il faudra quand même récupérer le nom qui n'est pas le sujet, pour aller l'utiliser en scrappant la page wiki du sujet\n",
    "                subject_dict['infos'].append(other_noun)\n",
    "            return(subject_dict)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "#test\n",
    "\n",
    "#doc = nlp(\"Who wrote The Lords of The Rings\")\n",
    "#for ents in doc.ents : \n",
    "    #print(ents.text, ents.label_)\n",
    "\n",
    "\n",
    "extract_subject_with_spacy('When was ancient greek used ?')\n",
    "extract_subject_with_spacy('When is Barack Obama born ?')\n",
    "extract_subject_with_spacy('WHAT IS THE LAST NAME OF THE AUTHOR WHO WROTE “ROMEO AND JULIET”?')\n",
    "extract_subject_with_spacy(\"What is the size of the moon?\")\n",
    "extract_subject_with_spacy(\"Who wrote The Lord of The Rings?\")\n",
    "extract_subject_with_spacy('What is the ?')\n",
    "extract_subject_with_spacy('What is the sky?')\n",
    "extract_subject_with_spacy('What is the color of the sky ?')\n",
    "#\n",
    "extract_subject_with_spacy('What is the meaning of \"bread\" ?') #pas de pb\n",
    "extract_subject_with_spacy('What is the meaning of \"omg\" ?') #les sigles ne sont pas reconnus\n",
    "extract_subject_with_spacy('What is the meaning of \"OMG\" ?') #ah bah en majuscule si\n",
    "extract_subject_with_spacy('What is the meaning of \"why\" ?') #cas (très) rare et spécial à gérer, quand une question porte sur un mot interrogatif qui est dans osef_list (faudra mettre une condition genre si y'a + d'un mot interrogatif, ne supprimer que le premier de noun_list)\n",
    "extract_subject_with_spacy('What is the third color of the french flag ?') #propre\n",
    "extract_subject_with_spacy(\"What is the color of Nirvana's second album ?\") #stylé que ça trouve le sujet dans ce genre de cas. Une future grosse tâche : trouver les articles wikipédia à partir de ce genre de paraphrase\n",
    "extract_subject_with_spacy('When are hops added in the brewing process ?')\n",
    "extract_subject_with_spacy('From what direction does the sun rise in the morning ?')\n",
    "extract_subject_with_spacy(\"What is the height of Nirvana's singer ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is Barack Obama born ? \n",
      " Wh ?  True \n",
      " Yes/No ?  False \n",
      " Pseudocleft ?  False \n",
      " Tutorial ? False \n",
      "\n",
      "Is the moon bigger than Mars ? \n",
      " Wh ?  False \n",
      " Yes/No ?  True \n",
      " Pseudocleft ?  False \n",
      " Tutorial ? False \n",
      "\n",
      "What she says is true.  \n",
      " Wh ?  True \n",
      " Yes/No ?  False \n",
      " Pseudocleft ?  True \n",
      " Tutorial ? False \n",
      "\n",
      "How to unlock a closed door ? \n",
      " Wh ?  True \n",
      " Yes/No ?  False \n",
      " Pseudocleft ?  False \n",
      " Tutorial ? True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def is_wh (question) : \n",
    "    doc = nlp(question)\n",
    "    interrogatives = ['where', 'who','why','what','when','which','how', 'Where', 'Who','Why','What','When','Which', 'How']\n",
    "    words = [word for word in doc]\n",
    "    if words[0].pos_ == 'SPACE' : #si le premier token est un espace, on le supprime pour le traitement\n",
    "        words = words[1:]\n",
    "    \n",
    "    if words[0].text in interrogatives or words[1].text in interrogatives : #si le premier ou deuxième mot de la question est un mot interrogatif, c'est une question en \"wh-\" : ex \"When is Barack Obama born?\", \"In which country is Paris ?\"\n",
    "        return(True)\n",
    "    else : \n",
    "        return(False) \n",
    "\n",
    "def is_polar(question) : #questions yes/no\n",
    "    doc = nlp(question)\n",
    "    words = [word for word in doc]\n",
    "    if words[0].pos_ == 'SPACE' : \n",
    "        words = words[1:]\n",
    "    \n",
    "    if words[0].pos_ == 'AUX' : #si la question commence par un auxiliaire, c'est une question yes/no\n",
    "        return(True)\n",
    "    else : \n",
    "        return(False)\n",
    "    \n",
    "\n",
    "def is_pseudocleft (question) : #détecter les pseudocleft -> phrases qui ressemblent à des questions mais n'en sont pas : \"Who is the President of Nicaragua doesn't interest me.\"    \n",
    "    doc = nlp(question) \n",
    "    interrogatives = ['where', 'who','why','what','when','which','how', 'Where', 'Who','Why','What','When','Which', 'How']\n",
    "    words = [word for word in doc]\n",
    "    if words[0].pos_ == 'SPACE' : \n",
    "        words = words[1:]\n",
    "    \n",
    "    if words[0].text in interrogatives and words[0].head.dep_ in [\"csubj\", \"advcl\"] : #si la dépendence head du mot interrogatif est un adverbial clause ou un subject clause (en gros le sujet ou l'adverbe sont des clauses, genre le sujet de la question c'est le \"What\", comme dans la phrase \"What she says is true\"), alors on est face à un pseudocleft (ou pseudo-wh) -> c'est une règle linguistique        \n",
    "        return(True) \n",
    "    else : \n",
    "        return(False)\n",
    "\n",
    "def is_tutorial (question) : #détecter les questions tutorielles\n",
    "    doc = nlp(question)\n",
    "    modals = ['to', 'can', 'could', 'shall', 'should']\n",
    "    words = [word for word in doc]\n",
    "    if words[0].text == \"How\" and words[1].text in modals  : \n",
    "        return(True)\n",
    "    else : \n",
    "        return(False)\n",
    "    \n",
    "\n",
    "#def is_toapp (question) : #détecte les questions adressées à l'app\n",
    "\n",
    "\n",
    "    \n",
    "#tests    \n",
    "questions = [\n",
    "            'Where is Barack Obama born ?',\n",
    "            'Is the moon bigger than Mars ?',\n",
    "            'What she says is true. ',\n",
    "            'How to unlock a closed door ?'\n",
    "            ]\n",
    "\n",
    "for question in questions : \n",
    "    print(question, '\\n' , \n",
    "          'Wh ? ', is_wh(question), '\\n', \n",
    "          'Yes/No ? ', is_polar(question), '\\n',\n",
    "          'Pseudocleft ? ', is_pseudocleft(question), '\\n',\n",
    "          'Tutorial ?', is_tutorial(question), '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Wikipedia API\n",
    "Try to found the most relevant context text to give as BERT input. <br>\n",
    "Get a wikipedia article, and scrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == WIP ==\n",
    "# découpé en paragraphe (le model a une limite de 512 token pour le text en entré)\n",
    "def get_wiki_and_split(subject):\n",
    "    text = wikipedia.summary(subject)\n",
    "    print(len(text))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia.search(\"salsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = wikipedia.summary('salsa')\n",
    "#list = text.split('.')\n",
    "#print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e29aad5be746589414181b013ea0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(height='0px', margin='100px 0 0 100px', width='400px')), Button(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef4767645cf4503ac341c3b27fa4c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='auto', margin='50px 0 100px 100px', width='450px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "#Widgets layout difinition\n",
    "layout = widgets.Layout(width='400px', height='0px', margin='100px 0 0 100px')\n",
    "bLayout = widgets.Layout(width='50px', height='28px', margin='100px 0 0 0px')\n",
    "outLayoutPropre = widgets.Layout(width='480px', height='100px', margin='50px 0 100px 100px')\n",
    "outLayoutTest = widgets.Layout(width='450px', height='auto', margin='50px 0 100px 100px')\n",
    "#titleLayout = widgets.Layout(width='450px', height='auto', margin='0px 0 0px 100px')\n",
    " \n",
    "#Widgets object definition\n",
    "text = widgets.Text(layout=layout)\n",
    "button = widgets.Button(description = 'Ask', layout = bLayout)\n",
    "out = widgets.Output(layout=outLayoutTest)#layout=outLayout\n",
    "#out = widgets.HTML(layout = outLayout, value= '<style>.text {width: 480px; heigh: 100px;}</style> <p class=\"text\">'+ out_value +' </p>')\n",
    " \n",
    "def button_on_click(self):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        subject = extract_subject_with_spacy(question=text.value)['subject']\n",
    "        if subject is not None:\n",
    "            context = wikipedia.summary(subject)\n",
    "            \n",
    "            answer = generate_answer(text.value, context[:2000])\n",
    "            #out.clear_output()\n",
    "            print(\"Here is what i found: \\n\"+ answer)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "button.on_click(button_on_click)\n",
    "\n",
    "display(widgets.HBox((text, button,)))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
