{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "\n",
    "import pickle\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  8.08325330000001\n"
     ]
    }
   ],
   "source": [
    "start()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.ask', 'wb'))\n",
    "pickle.dump(model, open('model.ask', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  2.0859068000000036\n"
     ]
    }
   ],
   "source": [
    "start()\n",
    "tokenizer = pickle.load(open('tokenizer.ask', 'rb'))\n",
    "model = pickle.load(open('model.ask', 'rb'))\n",
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Model function\n",
    "Use loaded BERT model to return answer<br>\n",
    "**Input** : Question, Context text <br>\n",
    "**return** : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    #print(\"I'm looking for an aswer, wait please ...\")\n",
    "    # == Tokenize ==\n",
    "    # use a python dictonary so run on CPU\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    #print(\"-Tokenization\")\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    #print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    # by default on CPU, use model.to(device) to select GPU !?\n",
    "    #print(\"-Forward pass on the model\")\n",
    "    \n",
    "    #move tensor to device\n",
    "    input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "    segment_ids_tensor = torch.tensor([segment_ids]).to(device)\n",
    "    \n",
    "    start_scores, end_scores = model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                 token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from \n",
    "    # ici retourne un score pour chaque token\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    # RÃ©cupÃ©re le socre\n",
    "    start_score = float(start_scores[0,answer_start])\n",
    "    end_score = float(end_scores[0,answer_end])\n",
    "    \n",
    "    \n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer, start_score+end_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm looking for an aswer, wait please ...\n",
      "-Tokenization\n",
      "-Forward pass on the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('barack obama', 6.05471658706665)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"Who is Barack Obama\", \"Barack Obama is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, Obama was the first African-American president of the United States. He previously served as a U.S. senator from Illinois from 2005 to 2008 and an Illinois state senator from 1997 to 2004.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2123, 1005, 1056, 2113, 102, 1057, 1012, 1047, 3475, 1005, 1056, 1037, 1037, 25212, 2063, 25212, 2518, 102]\n",
      "['[CLS]', 'i', 'don', \"'\", 't', 'know', '[SEP]', 'u', '.', 'k', 'isn', \"'\", 't', 'a', 'a', 'ee', '##e', 'ee', 'thing', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"i don't know\", \"U.K isn't a a eee ee thing\")\n",
    "print(input_ids)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Question Processing\n",
    "Extract subjet _(and more?)_ from the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subj(question):\n",
    "    subject = None\n",
    "    token = nltk.word_tokenize(question)\n",
    "    #print(token)\n",
    "    pos_token = nltk.pos_tag(token)\n",
    "    for item in pos_token:\n",
    "        if item[1] == 'NN':\n",
    "            subject = item[0]\n",
    "    if subject is not None:\n",
    "        print(\"Subject found: \" + subject)\n",
    "    else:\n",
    "        print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "    return  subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'truecase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2d00334485f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtruecase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'truecase'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import truecase\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_subject(question):\n",
    "    #noun to not take in count\n",
    "    subj_list = []\n",
    "    osef_list = ['who','why','what','when','which','how']\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    ents = doc.ents\n",
    "    nouns = doc.noun_chunks\n",
    "    # == Check for Named Entity ==\n",
    "    if len(ents) != 0:\n",
    "        for ent in doc.ents:\n",
    "            subj_list.append(ent.text)\n",
    "            print(\"Named entity: \" + ent.text) #, ent.start_char, ent.end_char, ent.label_\n",
    "    # == Check for Noun Chunk ==\n",
    "    elif len(list(nouns)) != 0:\n",
    "        for item in nouns:\n",
    "            if str(item) not in osef_list:\n",
    "                subj_list.append(str(item))\n",
    "                print(\"Noun chunck: \" + str(item))\n",
    "                \n",
    "    #print(\"Subject found: \" + str(item))\n",
    "    #return str(item)\n",
    "    #print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "    if len(subj_list) != 0:\n",
    "        print(\"Subject selected: \" + subj_list[-1])\n",
    "        return subj_list[-1]\n",
    "    else:\n",
    "        print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "        return None\n",
    "#test\n",
    "#extract_subject_with_spacy('which is the most common use of opt-in e-mail marketing ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how the dog dies?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_subject' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a106da6908ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mextract_subject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_subject' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"how the dog dies?\"\n",
    "\n",
    "#question = truecase.get_true_case(question)\n",
    "print(question)\n",
    "\n",
    "extract_subject(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"In\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "Try to found the most relevant context text to give as BERT input. <br>\n",
    "Get a wikipedia article, and scrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia #use to search article name\n",
    "import wikipediaapi #use to acess article page and content\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "import time\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Firt', 'senteces.']\n",
      "[\"I'dont\", 'if', 'U.K', 'is', 'in', 'Europe.']\n",
      "['But', 'osef', '.Lol']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Firt senteces. I'dont if U.K is in Europe. But osef .Lol\"\n",
    "doc = nlp(text)\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(sent.text)\n",
    "for s in sentences:\n",
    "    print(s.split(\" \"))\n",
    "#word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent computing\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Concurrent_computing'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_retrivial = wikipedia.search('computing programming')\n",
    "print(wiki_retrivial[0])\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        #extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "        #extract_format=wikipediaapi.ExtractFormat.HTML\n",
    "        )\n",
    "\n",
    "page = wiki.page(wiki_retrivial[0])\n",
    "print(page.exists())\n",
    "page.title\n",
    "#page.text\n",
    "#page.summary\n",
    "page.fullurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = 0\n",
    "\n",
    "def start():\n",
    "    global timer\n",
    "    timer = time.perf_counter()\n",
    "    \n",
    "def stop(message='Time: '):\n",
    "    global timer\n",
    "    print(message, time.perf_counter() - timer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections_list(page):\n",
    "    osef_list = ['Sources', 'Further reading', 'External links']\n",
    "    def get_sections(sections, sections_list, level=0):\n",
    "            for s in sections:\n",
    "                    #print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, len(s.text)))\n",
    "                    #check if there is text and if section is usefull\n",
    "                    if len(s.text) != 0 and s.title not in osef_list:\n",
    "                        sections_list.append(s.text)\n",
    "                    get_sections(s.sections, sections_list, level + 1)\n",
    "                    \n",
    "    sections_list = []\n",
    "    sections_list.append(page.summary)\n",
    "    get_sections(page.sections, sections_list)\n",
    "    return sections_list\n",
    "\n",
    "def get_paragraph(page):\n",
    "    result = []\n",
    "    result = get_sections_list(page)\n",
    "    \n",
    "    paragraph = []\n",
    "    for section in result:\n",
    "        for item in section.split(\"\\n\"):\n",
    "            #check len <512 // 400-450\n",
    "            if len(word_tokenize(item)) < 400:\n",
    "                paragraph.append(item)\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7c1d57b555ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "print(paragraph[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hippopotamus',\n",
       " 'Augustine of Hippo',\n",
       " 'Pygmy hippopotamus',\n",
       " 'House Hippo',\n",
       " 'Hippo Regius',\n",
       " 'Hippo Campus',\n",
       " 'Hippo CMS',\n",
       " 'Hungry Hungry Hippos',\n",
       " 'Hippo (disambiguation)',\n",
       " 'Hippos']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search('hippo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Time:  0.23894789999999944\n",
      "Time:  0.3071918999999923\n",
      "Model run Time:  2.9706179000000077\n",
      "9.52122974395752\n",
      "hippos mark their territory by defecation\n",
      "Hippos mark their territory by defecation.  While depositing the faeces, hippos spin their tails to distribute their excrement over a greater area. \"Yawning\" serves as a threat display. When fighting, males use their incisors to block each other's attacks and their large canines to inflict injuries.  When hippos become over-populated or a habitat is reduced, males sometimes attempt infanticide, but this behaviour is not common under normal conditions. Incidents of hippo cannibalism have been documented, but this is believed to be the behaviour of distressed or sick hippos.Hippos appear to communicate vocally, through grunts and bellows, and they may practice echolocation, but the purpose of these vocalisations is currently unknown. Hippos have the unique ability to hold their heads partially above the water and send out a cry that travels through both water and air; individuals respond above and under water. Hippos will also express threat and alarm with exhalations.\n",
      "https://en.wikipedia.org/wiki/Hippopotamus\n"
     ]
    }
   ],
   "source": [
    "# == Loop == \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start()\n",
    "page = wiki.page(wikipedia.search('hippo')[0])\n",
    "print(page.exists())\n",
    "stop()\n",
    "start()\n",
    "\n",
    "paragraph = get_paragraph(page)\n",
    "\n",
    "stop()\n",
    "\n",
    "answers = []\n",
    "scores = []\n",
    "timer = time.perf_counter() \n",
    "for p in paragraph:\n",
    "    answer, score = generate_answer(\"how a hippo scratches\", p)\n",
    "    answers.append(answer)\n",
    "    scores.append(score)\n",
    "print(\"Model run Time: \", time.perf_counter() - timer)\n",
    "    \n",
    "max_value = max(scores)\n",
    "\n",
    "index = scores.index(max_value)\n",
    "print(max_value)\n",
    "print(answers[index])\n",
    "print(paragraph[index])\n",
    "print(page.fullurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18.04034423828125, 'gdscript'),\n",
       " (15.488117694854736, 'object - oriented'),\n",
       " (15.314013481140137, 'java'),\n",
       " (14.860849380493164, 'julia'),\n",
       " (14.193211555480957, 'coffeescript'),\n",
       " (13.607266902923584, 'go'),\n",
       " (13.453736782073975, 'java'),\n",
       " (13.433351516723633, 'common lisp , scheme , or ruby'),\n",
       " (13.278484344482422, 'c or pascal'),\n",
       " (13.127068996429443, 'ecmascript / javascript')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(scores, answers), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python (programming language)\n"
     ]
    }
   ],
   "source": [
    "wiki_retrivial = wikipedia.search('python')\n",
    "print(wiki_retrivial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"Star Wars\"\n",
    "search = wikipedia.search(subj)\n",
    "print(search)\n",
    "#wikipedia.suggest(\"Lil Pump\")\n",
    "#print(wikipedia.suggest(\"OMG\"))\n",
    "page = wikipedia.page(search[0])\n",
    "page.title\n",
    "#'\\n'\n",
    "#page.content\n",
    "#page.url\n",
    "#page.title\n",
    "#page.images[0]\n",
    "\n",
    "#try:\n",
    "#    page = wikipedia.page(\"OMG (disambiguation)\")\n",
    "#except wikipedia.exceptions.DisambiguationError as e:\n",
    "#    print(e.options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikipedia.summary('star wars')\n",
    "#list = text.split('.')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8fd6ee4f1b4324a69b33f9bc2990ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(height='0px', margin='100px 0 0 100px', width='400px')), Button(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d755f9898aa64ecb91649ae2a59ae0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='auto', margin='50px 0 100px 100px', width='450px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "#Widgets layout difinition\n",
    "layout = widgets.Layout(width='400px', height='0px', margin='100px 0 0 100px')\n",
    "bLayout = widgets.Layout(width='50px', height='28px', margin='100px 0 0 0px')\n",
    "outLayoutPropre = widgets.Layout(width='480px', height='100px', margin='50px 0 100px 100px')\n",
    "outLayoutTest = widgets.Layout(width='450px', height='auto', margin='50px 0 100px 100px')\n",
    "#titleLayout = widgets.Layout(width='450px', height='auto', margin='0px 0 0px 100px')\n",
    " \n",
    "#Widgets object definition\n",
    "text = widgets.Text(layout=layout)\n",
    "button = widgets.Button(description = 'Ask', layout = bLayout)\n",
    "out = widgets.Output(layout=outLayoutTest)#layout=outLayout\n",
    "#out = widgets.HTML(layout = outLayout, value= '<style>.text {width: 480px; heigh: 100px;}</style> <p class=\"text\">'+ out_value +' </p>')\n",
    " \n",
    "def button_on_click(self):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "        subject = extract_subject(question=text.value)\n",
    "        \n",
    "        if subject is not None:\n",
    "            \n",
    "            context = wikipedia.summary(subject)\n",
    "            \n",
    "            answer, score = generate_answer(text.value, context[:2000])\n",
    "            #out.clear_output()\n",
    "            print()\n",
    "            print(\"Here what i found: \\n\"+ answer)\n",
    "            print()\n",
    "            print(\"Score: \" +str(score))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "button.on_click(button_on_click)\n",
    "\n",
    "display(widgets.HBox((text, button,)))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
