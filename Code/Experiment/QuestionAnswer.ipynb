{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Model function\n",
    "Use loaded BERT model to return answer<br>\n",
    "**Input** : Question, Context text <br>\n",
    "**return** : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    print(\"I'm looking for an aswer, wait please ...\")\n",
    "    # == Tokenize ==\n",
    "    # use a python dictonary so run on CPU\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    print(\"-Tokenization\")\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    #print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    # by default on CPU, use model.to(device) to select GPU !?\n",
    "    print(\"-Forward pass on the model\")\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from \n",
    "    # ici retourne un score pour chaque token\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    # R√©cup√©re le socre\n",
    "    start_score = float(start_scores[0,answer_start])\n",
    "    end_score = float(end_scores[0,answer_end])\n",
    "    \n",
    "    \n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer, start_score+end_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm looking for an aswer, wait please ...\n",
      "-Tokenization\n",
      "-Forward pass on the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('initial public offering', 14.472158908843994)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"What is IPO?\", \"Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering (IPO) took place on August 19, 2004, and Google moved to its headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google is Alphabet's leading subsidiary and will continue to be the umbrella company for Alphabet's Internet interests. Sundar Pichai was appointed CEO of Google, replacing Larry Page who became the CEO of Alphabet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Question Processing\n",
    "Extract subjet _(and more?)_ from the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subj(question):\n",
    "    subject = None\n",
    "    token = nltk.word_tokenize(question)\n",
    "    #print(token)\n",
    "    pos_token = nltk.pos_tag(token)\n",
    "    for item in pos_token:\n",
    "        if item[1] == 'NN':\n",
    "            subject = item[0]\n",
    "    if subject is not None:\n",
    "        print(\"Subject found: \" + subject)\n",
    "    else:\n",
    "        print(\"Subject not found üòî\\n Rephrase the question or try another one\")\n",
    "    return  subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import truecase\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_subject(question):\n",
    "    #noun to not take in count\n",
    "    subj_list = []\n",
    "    osef_list = ['who','why','what','when','which','how']\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    ents = doc.ents\n",
    "    nouns = doc.noun_chunks\n",
    "    # == Check for Named Entity ==\n",
    "    if len(ents) != 0:\n",
    "        for ent in doc.ents:\n",
    "            subj_list.append(ent.text)\n",
    "            print(\"Named entity: \" + ent.text) #, ent.start_char, ent.end_char, ent.label_\n",
    "    # == Check for Noun Chunk ==\n",
    "    elif len(list(nouns)) != 0:\n",
    "        for item in nouns:\n",
    "            if str(item) not in osef_list:\n",
    "                subj_list.append(str(item))\n",
    "                print(\"Noun chunck: \" + str(item))\n",
    "                \n",
    "    #print(\"Subject found: \" + str(item))\n",
    "    #return str(item)\n",
    "    #print(\"Subject not found üòî\\n Rephrase the question or try another one\")\n",
    "    if len(subj_list) != 0:\n",
    "        print(\"Subject selected: \" + subj_list[-1])\n",
    "        return subj_list[-1]\n",
    "    else:\n",
    "        print(\"Subject not found üòî\\n Rephrase the question or try another one\")\n",
    "        return None\n",
    "#test\n",
    "#extract_subject_with_spacy('which is the most common use of opt-in e-mail marketing ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Darth Vader dies?\n",
      "<class 'tuple'>\n",
      "<class 'generator'>\n",
      "Named entity: Darth Vader\n",
      "Subject selected: Darth Vader\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Darth Vader'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how darth vader dies?\"\n",
    "\n",
    "question = truecase.get_true_case(question)\n",
    "print(question)\n",
    "\n",
    "extract_subject(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"In\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "Try to found the most relevant context text to give as BERT input. <br>\n",
    "Get a wikipedia article, and scrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == WIP ==\n",
    "# d√©coup√© en paragraphe (le model a une limite de 512 token pour le text en entr√©)\n",
    "def get_wiki_and_split(subject):\n",
    "    text = wikipedia.summary(subject)\n",
    "    print(len(text))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mace Windu',\n",
       " 'Star Wars: The Clone Wars (2002 video game)',\n",
       " 'Star Wars: Clone Wars (2003 TV series)',\n",
       " 'Star Wars: The Rise of Skywalker',\n",
       " 'Terrence C. Carson',\n",
       " 'List of Star Wars: The Clone Wars cast members',\n",
       " 'Star Wars: The Clone Wars (2008 TV series)',\n",
       " 'Jango Fett',\n",
       " 'Star Wars: Episode III ‚Äì Revenge of the Sith',\n",
       " 'Star Wars: Episode II ‚Äì Attack of the Clones']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search(\"star wars mace windu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Samba (Portuguese pronunciation: [Ààs…êÃÉb…ê] (listen)), also known as samba urbano carioca (Urban Carioca Samba) or simply samba carioca (Carioca Samba) is a Brazilian music genre that originated in the Afro-Brazilians communities of Rio de Janeiro in the early 20th century', ' Having its roots in the cultural expression of West Africa and in Brazilian folk traditions, especially those linked to the primitive rural samba of the colonial and imperial periods, is considered one of the most important cultural phenomena in Brazil and one of the country symbols Present in the Portuguese language at least since the 19th century, the word ‚Äúsamba‚Äù was originally used to designate a ‚Äúpopular dance‚Äù', ' Over time, its meaning has been extended to a ‚Äúbatuque-like circle dance‚Äù, a dance style and also to a ‚Äúmusic genre‚Äù', ' This process of establishing itself as a musical genre began in the 1910s and it had its inaugural landmark in the song ‚ÄúPelo Telefone‚Äù, launched in 1917', ' Despite being identified by its creators, the public and the Brazilian music industry as ‚Äúsamba‚Äù, this pioneering style was much more connected from the rhythmic and instrumental point of view to maxixe than to samba itself', 'Samba was modernly structured as a musical genre only in the late 1920s from the neighborhood of Est√°cio and soon extended to Oswaldo Cruz and other parts of Rio through its commuter rail', ' Today synonymous with the rhythm of samba, this new samba brought innovations in rhythm, melody and also in thematic aspects', ' Its rhythmic change based on a new percussive instrumental pattern resulted in a more ‚Äúbatucado‚Äù and syncopated style ‚Äì as opposed to the inaugural ‚Äúsamba-maxixe‚Äù ‚Äì notably characterized by a faster tempo, longer notes and a characterized cadence far beyond the simple ones palms used so far', ' Also the ‚ÄúEst√°cio paradigm‚Äù innovated in the formatting of samba as a song, with its musical organization in first and second parts in both melody and lyrics', ' In this way, the sambistas of Est√°cio created, structured and redefined the Urban Carioca Samba as a genre in a modern and finished way', ' In this process of establishment as an urban and modern musical expression, the Carioca Samba had the decisive role of samba schools, responsible for defining and legitimizing definitively the aesthetic bases of rhythm, and radio broadcasting, which greatly contributed to the diffusion and popularization of the genre and its song singers', ' Thus, samba has achieved major projection throughout Brazil and has become one of the main symbols of Brazilian national identity', \" Once criminalized and viewed with prejudice for its Afro-Brazilian origins, the genre has also conquered support between members of the most favored classes and the country's cultural elite\", 'At the same time that it established itself as the genesis of samba, the ‚ÄúEst√°cio paradigm‚Äù paved the way for its fragmentation into new sub-genres and styles of composition and interpretation throughout the 20th century', ' Mainly from the so-called ‚Äúgolden age‚Äù of Brazilian music, samba received abundant categorizations, some of which denote solid and well-accepted derivative strands ‚Äì such as bossa nova, pagode, partido alto, samba de breque, samba-can√ß√£o, samba de enredo and samba de terreiro ‚Äì while other nomenclatures were somewhat more imprecise ‚Äì such as samba do barulho (literally ‚Äúnoise samba‚Äù), samba epistolar (‚Äúepistolary samba‚Äù) ou samba fon√©tico (‚Äúphonetic samba‚Äù) ‚Äì and some merely derogatory ‚Äì such as sambalada, sambolero or samb√£o joia', 'The modern samba that emerged at the beginning of the 20th century is predominantly in a 2/4 time signature varied with the conscious use of a sung chorus to a batucada rhythm, with various stanzas of declaratory verses', ' Its traditional instrumentation is composed of percussion instruments such as the pandeiro, cu√≠ca, tamborim, ganz√° and surdo accompaniment ‚Äì whose inspiration is choro ‚Äì such as classical guitar and cavaquinho', ' In 2007, the Brazilian National Institute of Historic and Artistic Heritage declared Carioca Samba and three of its matrixes ‚Äì samba de terreiro, partido-alto and samba de enredo ‚Äì as cultural heritage in Brazil', '']\n"
     ]
    }
   ],
   "source": [
    "text = wikipedia.summary('salsa')\n",
    "list = text.split('.')\n",
    "print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6fb0df43d2493fb8d7cf453752bd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(height='0px', margin='100px 0 0 100px', width='400px')), Button(de‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485f8e1dc9b24377b710681dfa06f20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='auto', margin='50px 0 100px 100px', width='450px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "#Widgets layout difinition\n",
    "layout = widgets.Layout(width='400px', height='0px', margin='100px 0 0 100px')\n",
    "bLayout = widgets.Layout(width='50px', height='28px', margin='100px 0 0 0px')\n",
    "outLayoutPropre = widgets.Layout(width='480px', height='100px', margin='50px 0 100px 100px')\n",
    "outLayoutTest = widgets.Layout(width='450px', height='auto', margin='50px 0 100px 100px')\n",
    "#titleLayout = widgets.Layout(width='450px', height='auto', margin='0px 0 0px 100px')\n",
    " \n",
    "#Widgets object definition\n",
    "text = widgets.Text(layout=layout)\n",
    "button = widgets.Button(description = 'Ask', layout = bLayout)\n",
    "out = widgets.Output(layout=outLayoutTest)#layout=outLayout\n",
    "#out = widgets.HTML(layout = outLayout, value= '<style>.text {width: 480px; heigh: 100px;}</style> <p class=\"text\">'+ out_value +' </p>')\n",
    " \n",
    "def button_on_click(self):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "        subject = extract_subject(question=text.value)\n",
    "        \n",
    "        if subject is not None:\n",
    "            \n",
    "            context = wikipedia.summary(subject)\n",
    "            \n",
    "            answer, score = generate_answer(text.value, context[:2000])\n",
    "            #out.clear_output()\n",
    "            print()\n",
    "            print(\"Here what i found: \\n\"+ answer)\n",
    "            print()\n",
    "            print(\"Score: \" +str(score))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "button.on_click(button_on_click)\n",
    "\n",
    "display(widgets.HBox((text, button,)))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
