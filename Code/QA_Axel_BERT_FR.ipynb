{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import fr_core_news_md\n",
    "import wikipedia\n",
    "wikipedia.set_lang(\"fr\")\n",
    "import wikipediaapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def class Ask / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ask:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        self.model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "        #move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.nlp = en_core_web_sm.load()\n",
    "        self.wiki = wikipediaapi.Wikipedia(language='fr')\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def generateAnswer(self, question, answer_text):\n",
    "        # == Tokenize == Apply the tokenizer to the input text, treating them as a text-pair. (CPU)\n",
    "        input_ids = self.tokenizer.encode(question, answer_text)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        # == Set Segment IDs == Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(self.tokenizer.sep_token_id)\n",
    "        # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "        # == Run Model == Run our example through the model. (GPU)\n",
    "        #move tensor to device\n",
    "        input_ids_tensor = torch.tensor([input_ids]).to(self.device)\n",
    "        segment_ids_tensor = torch.tensor([segment_ids]).to(self.device)\n",
    "    \n",
    "        start_scores, end_scores = self.model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                 token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from \n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        # get score\n",
    "        start_score = float(start_scores[0,answer_start])\n",
    "        end_score = float(end_scores[0,answer_end])\n",
    "    \n",
    "    \n",
    "        # == Print Answer without ## ==\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "    \n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        return answer, start_score+end_score\n",
    "\n",
    "    def extract_subject_with_spacy(self, question):\n",
    "    \n",
    "        #question = truecase.get_true_case(question) #le truecaser est un peu bidon j'ai l'impression\n",
    "        #print(question)\n",
    "        \n",
    "        subject_dict = {'subject' : '', 'infos' : []} #dictionnaire qui contiendra le sujet, et les infos complémentaires\n",
    "        \n",
    "        osef_list = ['qui','pourquoi','quoi','quand','quel','quelle','comment','où','Qui','Pourquoi','Quoi','Quand','Quel','Quelle', 'Comment']\n",
    "        doc = self.nlp(question)\n",
    "        \n",
    "        #on prépare une liste des noms communs (ou plus précisent chunks, qui peuvent être des groupes nominaux plus larges, des unités de sens) de la question\n",
    "        \n",
    "        nouns_list = []\n",
    "        #dep_list = []\n",
    "        for noun in doc.noun_chunks :\n",
    "            #dep_list.append(noun.root.dep_)\n",
    "            nouns_list.append(noun)\n",
    "               \n",
    "        #on enlève de la liste des potentiels sujets les mots interrogatifs venant de osef_list\n",
    "        for noun in nouns_list : \n",
    "            if str(noun) in osef_list : \n",
    "                nouns_list.remove(noun)\n",
    "                    \n",
    "        #on crée une liste d'entité nommées de la phrase. S'il y en a une dans la question, alors c'est le sujet\n",
    "        #nn crée également une liste qui va contenir les labels de ces entités nommées, car certains types d'entités ne nous intéressent pas\n",
    "        #les labels qui nous intéressent sont dans la liste relevant_labels\n",
    "        \n",
    "        #si il y a des entités nommées, on les utilise comme sujet et les chunks alentours comme infos supplémentaires\n",
    "        #si il n'y a pas d'entités nommées, on va uniquement regarder les chunks (dans le 'else')\n",
    "        \n",
    "        ents_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        relevant_labels = ['PERSON','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW']\n",
    "        for ent in doc.ents :\n",
    "            if ent.label_ in relevant_labels : \n",
    "                ents_list.append(ent.text)\n",
    "                labels_list.append(ent.label_)\n",
    "                #dep_list.append(ent.dep_) #pour tests sur depencies\n",
    "                \n",
    "        if ents_list and labels_list : \n",
    "            print(ents_list)\n",
    "            #print(dep_list)\n",
    "            print('subject found by ent : ', labels_list[-1] , ents_list[-1], '\\n')\n",
    "            subject_dict['subject'] = ents_list[-1] #on renvoie la dernière entité nommée pertinente trouvée\n",
    "            for other_noun in nouns_list : \n",
    "                subject_dict['infos'].append(other_noun)\n",
    "            return(subject_dict)\n",
    "            \n",
    "    \n",
    "        else : \n",
    "            \n",
    "    #si notre liste de chunks potentiels sujets est vide : pas de sujet\n",
    "    #si elle est égal à 1 : pas de doute, le sujet est cet élément\n",
    "    #si elle est plus grande que 1, le sujet est le deuxième élément\n",
    "    #règle simpliste mais qui semble suivre la logique de la formulation d'une question : c'est souvent le second nom qui est le sujet dans les questions qui en comportent deux, j'ai l'impression \n",
    "        \n",
    "            print(nouns_list)\n",
    "            #print(dep_list)\n",
    "            if(len(nouns_list)) == 0 :\n",
    "                print(\"subject not found, please try another formulation\", '\\n')\n",
    "            else :\n",
    "                print(\"subject found by noun: \" + str(nouns_list[-1]), '\\n')\n",
    "                subject_dict['subject'] = str(nouns_list[-1]) #le sujet est le dernier chunk\n",
    "                for other_noun in nouns_list[0:-1] : #dans ces cas de figure avec + d'un nom, il faudra quand même récupérer le nom qui n'est pas le sujet, pour aller l'utiliser en scrappant la page wiki du sujet\n",
    "                    subject_dict['infos'].append(other_noun)\n",
    "                return(subject_dict)\n",
    "\n",
    "    def get_sections_list(self, page):\n",
    "        osef_list = ['Sources', 'Further reading', 'External links']\n",
    "        def get_sections(sections, sections_list, level=0):\n",
    "                for s in sections:\n",
    "                        #print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, len(s.text)))\n",
    "                        #check if there is text and if section is usefull\n",
    "                        if len(s.text) != 0 and s.title not in osef_list:\n",
    "                            sections_list.append(s.text)\n",
    "                        get_sections(s.sections, sections_list, level + 1)\n",
    "                        \n",
    "        sections_list = []\n",
    "        sections_list.append(page.summary)\n",
    "        get_sections(page.sections, sections_list)\n",
    "        return sections_list\n",
    "\n",
    "    def get_paragraph(self, page):\n",
    "        result = []\n",
    "        result = self.get_sections_list(page)\n",
    "        \n",
    "        paragraph = []\n",
    "        for section in result:\n",
    "            for item in section.split(\"\\n\"):\n",
    "                #check len <512 // 400-450\n",
    "                if len(word_tokenize(item)) < 400:\n",
    "                    paragraph.append(item)\n",
    "        return paragraph\n",
    "\n",
    "    def get_best_answer(self, question, subject):\n",
    "        page = self.wiki.page(wikipedia.search(subject)[0])\n",
    "\n",
    "        paragraph = self.get_paragraph(page)\n",
    "        \n",
    "        \n",
    "        answers = []\n",
    "        scores = []\n",
    "\n",
    "        for p in paragraph[:15]:\n",
    "            answer, score = self.generateAnswer(question, p)\n",
    "            answers.append(answer)\n",
    "            scores.append(score)\n",
    "            \n",
    "        max_value = max(scores)\n",
    "        \n",
    "        index = scores.index(max_value)\n",
    "        return answers[index], paragraph[index], page.fullurl\n",
    "        \n",
    "\n",
    "    def run(self, question):\n",
    "        sujet = self.extract_subject_with_spacy(question)\n",
    "        a, p, u = self.get_best_answer(question, sujet['subject'])\n",
    "        return a, p, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Asker and Examples of the class method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "asker = ask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[le seigneur des anneaux]\n",
      "subject found by noun: le seigneur des anneaux \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('un roman en trois volumes de j . r . r . tolkien paru en 1954 et 1955',\n",
       " 'Le Seigneur des anneaux (The Lord of the Rings) est un roman en trois volumes de J. R. R. Tolkien paru en 1954 et 1955.',\n",
       " 'https://fr.wikipedia.org/wiki/Le_Seigneur_des_anneaux')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asker.run(\"Quand est sorti le seigneur des anneaux ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['François Ier']\n",
      "subject found by ent :  PRODUCT François Ier \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('dans la cathedrale de reims',\n",
       " \"François Ier (né sous le nom de François d’Angoulême le 12 septembre 1494 à Cognac et mort le 31 mars 1547 à Rambouillet) est sacré roi de France le 25 janvier 1515 dans la cathédrale de Reims. Il règne jusqu’à sa mort en 1547. Fils de Charles d'Orléans et de Louise de Savoie, il appartient à la branche de Valois-Angoulême de la dynastie capétienne.\",\n",
       " 'https://fr.wikipedia.org/wiki/Fran%C3%A7ois_Ier_(roi_de_France)')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asker.run(\"Où est sacré François Ier ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nicolas Sarkozy']\n",
      "subject found by ent :  PERSON Nicolas Sarkozy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('marie trois fois',\n",
       " \"Nicolas Sarkozy s'est marié trois fois et il est père de quatre enfants : Pierre (1985) et Jean (1986), nés de son mariage avec Marie-Dominique Culioli (mariés en 1982, divorcés en 1996), Louis (1997), né de son mariage avec Cécilia Ciganer-Albéniz (mariés en 1996, divorcés en 2007), et Giulia (2011), née de son mariage avec Carla Bruni-Tedeschi (le mariage a eu lieu le 2 février 2008 dans le Salon vert du palais de l'Élysée, sans publication des bans grâce à l'autorisation du procureur de la République, pour ne pas « troubler l'ordre public »). Son couple avec Cécilia fut largement médiatisé, y compris ses problèmes conjugaux en 2005-2007,,,.\",\n",
       " 'https://fr.wikipedia.org/wiki/Nicolas_Sarkozy')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asker.run(\"quand est élu Nicolas Sarkozy ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes : Extraction ent for wikipedia with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "À quelle fréquence l'employeur doit-il organiser des élections ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Droit de grève en France',\n",
       " 'Dreux',\n",
       " 'Pandémie de Covid-19 en France',\n",
       " 'Périgueux',\n",
       " 'Lognes',\n",
       " 'Saint-Omer (Pas-de-Calais)',\n",
       " 'Rennes',\n",
       " 'Villepinte (Seine-Saint-Denis)',\n",
       " 'Saint-Maurice-de-Beynost',\n",
       " 'Annemasse']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "question = \"Où sont détaillées les Heures Creuses ?\"\n",
    "\n",
    "query = \" \".join([str(x) for x in nlp(question).ents])\n",
    "query = query if len(query) > 0 else question\n",
    "\n",
    "print(query)\n",
    "\n",
    "relevant_title = wikipedia.search(query,results=10)\n",
    "\n",
    "relevant_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection with langdetect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(\"War doesn't show who's right, just who's left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Ein, zwei, drei, vier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Où sont détaillées les Heures Creuses ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
