{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "#from transformers import BertForQuestionAnswering\n",
    "#from transformers import BertTokenizer\n",
    "#MODIF\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "#FIN MODIF\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#MODIF\n",
    "import fr_core_news_md\n",
    "#FIN MODIF\n",
    "import wikipedia\n",
    "import wikipediaapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#MODIF (+ à installer)\n",
    "#python -m spacy download fr_core_news_md\n",
    "wikipedia.set_lang(\"fr\")\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "#FIN MODIF\n",
    "\n",
    "# si besoin clone the repo\n",
    "#git lfs install\n",
    "#git clone https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def class Ask / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class askfr:\n",
    "    def __init__(self):\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        #self.model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        #MODIF\n",
    "        ##self.tokenizer = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "        ##self.model = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "        \n",
    "        #FIN MODIF\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #self.device = ('cpu')\n",
    "        print(self.device)\n",
    "        #move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "        #self.wiki = wikipediaapi.Wikipedia(language='en')\n",
    "        #MODIF\n",
    "        #self.nlp = fr_core_news_md.load()   #en_core_web_sm is better\n",
    "        self.nlp = en_core_web_sm.load()\n",
    "        self.wiki = wikipediaapi.Wikipedia(language='fr')\n",
    "        self.nlppipeline = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')\n",
    "        #FIN MODIF\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def generateAnswer(self, question, answer_text):\n",
    "        # == Tokenize == Apply the tokenizer to the input text, treating them as a text-pair. (CPU)\n",
    "        input_ids = self.tokenizer.encode(question, answer_text)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        # == Set Segment IDs == Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(self.tokenizer.sep_token_id)\n",
    "        # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "        # == Run Model == Run our example through the model. (GPU)\n",
    "        #move tensor to device\n",
    "        input_ids_tensor = torch.tensor([input_ids]).to(self.device)\n",
    "        segment_ids_tensor = torch.tensor([segment_ids]).to(self.device)\n",
    "    \n",
    "    \n",
    "        #print(input_ids_tensor)\n",
    "        #print(input_ids_tensor.shape)\n",
    "        #print(input_ids_tensor.size)\n",
    "        #print(\"\")\n",
    "        #print(segment_ids_tensor)\n",
    "        #print(segment_ids_tensor.shape)\n",
    "        #print(segment_ids_tensor.size)\n",
    "        \n",
    "        start_scores, end_scores = self.model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                 token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from \n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        # get score\n",
    "        start_score = float(start_scores[0,answer_start])\n",
    "        end_score = float(end_scores[0,answer_end])\n",
    "    \n",
    "    \n",
    "        # == Print Answer without ## ==\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "    \n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        return answer, start_score+end_score\n",
    "\n",
    "    def extract_subject_with_spacy(self, question):\n",
    "    \n",
    "        #question = truecase.get_true_case(question) #le truecaser est un peu bidon j'ai l'impression\n",
    "        #print(question)\n",
    "        \n",
    "        subject_dict = {'subject' : '', 'infos' : []} #dictionnaire qui contiendra le sujet, et les infos complémentaires\n",
    "        \n",
    "        #osef_list = ['who','why','what','when','which','how', 'Who','Why','What','When','Which', 'How'] #noun to not take into account\n",
    "        #MODIF\n",
    "        osef_list = ['qui','pourquoi','quoi','quand','quel','quelle','comment','où','Qui','Pourquoi','Quoi','Quand','Quel','Quelle', 'Comment']\n",
    "        #FIN MODIF\n",
    "        doc = self.nlp(question)\n",
    "        \n",
    "        #on prépare une liste des noms communs (ou plus précisent chunks, qui peuvent être des groupes nominaux plus larges, des unités de sens) de la question\n",
    "        \n",
    "        nouns_list = []\n",
    "        #dep_list = []\n",
    "        for noun in doc.noun_chunks :\n",
    "            #dep_list.append(noun.root.dep_)\n",
    "            nouns_list.append(noun)\n",
    "               \n",
    "        #on enlève de la liste des potentiels sujets les mots interrogatifs venant de osef_list\n",
    "        for noun in nouns_list : \n",
    "            if str(noun) in osef_list : \n",
    "                nouns_list.remove(noun)\n",
    "                    \n",
    "        #on crée une liste d'entité nommées de la phrase. S'il y en a une dans la question, alors c'est le sujet\n",
    "        #nn crée également une liste qui va contenir les labels de ces entités nommées, car certains types d'entités ne nous intéressent pas\n",
    "        #les labels qui nous intéressent sont dans la liste relevant_labels\n",
    "        \n",
    "        #si il y a des entités nommées, on les utilise comme sujet et les chunks alentours comme infos supplémentaires\n",
    "        #si il n'y a pas d'entités nommées, on va uniquement regarder les chunks (dans le 'else')\n",
    "        \n",
    "        ents_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        relevant_labels = ['PERSON','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW']\n",
    "        for ent in doc.ents : #TENTER SPACY FR \n",
    "            if ent.label_ in relevant_labels : \n",
    "                ents_list.append(ent.text)\n",
    "                labels_list.append(ent.label_)\n",
    "                #dep_list.append(ent.dep_) #pour tests sur depencies\n",
    "                \n",
    "        if ents_list and labels_list : \n",
    "            print(ents_list)\n",
    "            #print(dep_list)\n",
    "            print('subject found by ent : ', labels_list[-1] , ents_list[-1], '\\n')\n",
    "            subject_dict['subject'] = ents_list[-1] #on renvoie la dernière entité nommée pertinente trouvée\n",
    "            for other_noun in nouns_list : \n",
    "                subject_dict['infos'].append(other_noun)\n",
    "            return(subject_dict)\n",
    "            \n",
    "    \n",
    "        else : \n",
    "            \n",
    "    #si notre liste de chunks potentiels sujets est vide : pas de sujet\n",
    "    #si elle est égal à 1 : pas de doute, le sujet est cet élément\n",
    "    #si elle est plus grande que 1, le sujet est le deuxième élément\n",
    "    #règle simpliste mais qui semble suivre la logique de la formulation d'une question : c'est souvent le second nom qui est le sujet dans les questions qui en comportent deux, j'ai l'impression \n",
    "        \n",
    "            print(nouns_list)\n",
    "            #print(dep_list)\n",
    "            if(len(nouns_list)) == 0 :\n",
    "                print(\"subject not found, please try another formulation\", '\\n')\n",
    "            else :\n",
    "                print(\"subject found by noun: \" + str(nouns_list[-1]), '\\n')\n",
    "                subject_dict['subject'] = str(nouns_list[-1]) #le sujet est le dernier chunk\n",
    "                for other_noun in nouns_list[0:-1] : #dans ces cas de figure avec + d'un nom, il faudra quand même récupérer le nom qui n'est pas le sujet, pour aller l'utiliser en scrappant la page wiki du sujet\n",
    "                    subject_dict['infos'].append(other_noun)\n",
    "                return(subject_dict)\n",
    "\n",
    "    def get_sections_list(self, page):\n",
    "        osef_list = ['Sources', 'Further reading', 'External links']\n",
    "        def get_sections(sections, sections_list, level=0):\n",
    "                for s in sections:\n",
    "                        #print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, len(s.text)))\n",
    "                        #check if there is text and if section is usefull\n",
    "                        if len(s.text) != 0 and s.title not in osef_list:\n",
    "                            sections_list.append(s.text)\n",
    "                        get_sections(s.sections, sections_list, level + 1)\n",
    "                        \n",
    "        sections_list = []\n",
    "        sections_list.append(page.summary)\n",
    "        get_sections(page.sections, sections_list)\n",
    "        return sections_list\n",
    "\n",
    "    def get_paragraph(self, page):\n",
    "        result = []\n",
    "        result = self.get_sections_list(page)\n",
    "        \n",
    "        paragraph = []\n",
    "        for section in result:\n",
    "            for item in section.split(\"\\n\"):\n",
    "                #check len <512 // 400-450\n",
    "                if len(word_tokenize(item)) < 400:\n",
    "                    paragraph.append(item)\n",
    "        return paragraph\n",
    "\n",
    "    def get_best_answer(self, question, subject):\n",
    "        page = self.wiki.page(wikipedia.search(subject)[0])\n",
    "\n",
    "        paragraph = self.get_paragraph(page)\n",
    "        \n",
    "        \n",
    "        answers = []\n",
    "        scores = []\n",
    "\n",
    "        for p in paragraph[:15]:\n",
    "            #answer, score = self.generateAnswer(question, p)\n",
    "            #answers.append(answer)\n",
    "            #scores.append(score)\n",
    "            #MODIF\n",
    "            nlpprocess = self.nlppipeline({'question': question,\n",
    "                                          'context': p})\n",
    "            answers.append(nlpprocess['answer'])\n",
    "            print(nlpprocess['answer'])\n",
    "            scores.append(nlpprocess['score'])\n",
    "            print(nlpprocess['score'])\n",
    "            #FIN MODIF\n",
    "            \n",
    "        max_value = max(scores)\n",
    "        \n",
    "        index = scores.index(max_value)\n",
    "        return answers[index], paragraph[index], page.fullurl\n",
    "        \n",
    "\n",
    "    def run(self, question):\n",
    "        sujet = self.extract_subject_with_spacy(question)\n",
    "        a, p, u = self.get_best_answer(question, sujet['subject'])\n",
    "        return a, p, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "askerfr = askfr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example extraction subject modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qui', 'Nicolas Sarkozy']\n",
      "subject found by ent :  PERSON Nicolas Sarkozy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject': 'Nicolas Sarkozy', 'infos': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askerfr.extract_subject_with_spacy(\"qui est Nicolas Sarkozy ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of the class method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qui', 'Nicolas Sarkozy']\n",
      "subject found by ent :  PERSON Nicolas Sarkozy \n",
      "\n",
      "Nicolas Sarközy de Nagy-Bocsa,\n",
      "0.26079830527305603\n",
      "président du conseil général des Hauts-de-Seine.\n",
      "0.15897059440612793\n",
      "président de la République\n",
      "0.4885251224040985\n",
      "Les Républicains.\n",
      "0.10274647176265717\n",
      "financement illicite de campagne électorale, recel de détournement de fonds publics libyens\n",
      "0.06022872030735016\n",
      "immigré hongrois, et d'Andrée Mallah\n",
      "0.060354795306921005\n",
      "fils d'un directeur général de la CIA,\n",
      "0.41663965582847595\n",
      "il est père de quatre enfants\n",
      "0.8887252807617188\n",
      "journaliste\n",
      "0.3655945360660553\n",
      "il entre à l'Institut d'études politiques de Paris\n",
      "0.017394894734025\n",
      "il appartient au Groupe rapide d'intervention,\n",
      "0.041067760437726974\n",
      "d'avocat d'affaires\n",
      "0.5350726246833801\n",
      "Arnaud Lagardère, qui le considère comme son « frère ».\n",
      "0.4383656978607178\n",
      "le fonds souverain du Qatar.\n",
      "0.461905837059021\n",
      "L'ancien chef de l'État,\n",
      "0.6162910461425781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('il est père de quatre enfants',\n",
       " \"Nicolas Sarkozy s'est marié trois fois et il est père de quatre enfants : Pierre (1985) et Jean (1986), nés de son mariage avec Marie-Dominique Culioli (mariés en 1982, divorcés en 1996), Louis (1997), né de son mariage avec Cécilia Ciganer-Albéniz (mariés en 1996, divorcés en 2007), et Giulia (2011), née de son mariage avec Carla Bruni-Tedeschi (le mariage a eu lieu le 2 février 2008 dans le Salon vert du palais de l'Élysée, sans publication des bans grâce à l'autorisation du procureur de la République, pour ne pas « troubler l'ordre public »). Son couple avec Cécilia fut largement médiatisé, y compris ses problèmes conjugaux en 2005-2007,,,.\",\n",
       " 'https://fr.wikipedia.org/wiki/Nicolas_Sarkozy')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askerfr.run(\"qui est Nicolas Sarkozy ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Napoléon Bonaparte]\n",
      "subject found by noun: Napoléon Bonaparte \n",
      "\n",
      "Ajaccio\n",
      "0.9691911935806274\n",
      "en la cathédrale Notre-Dame de Paris,\n",
      "0.25229862332344055\n",
      "d'Italie au Nil\n",
      "0.5392048954963684\n",
      "La France\n",
      "0.15998131036758423\n",
      "Naples\n",
      "0.9313117265701294\n",
      "Portugal,\n",
      "0.031144538894295692\n",
      "Sainte-Hélène,\n",
      "0.8043921589851379\n",
      "Sainte-Hélène,\n",
      "0.1900622844696045\n",
      "Ajaccio\n",
      "0.9015153050422668\n",
      "d'Ajaccio\n",
      "0.9577045440673828\n",
      "avocat au Conseil supérieur de l'île\n",
      "0.4956130087375641\n",
      "15 août, un jour férié : la Saint-Napoléon.\n",
      "0.17052310705184937\n",
      "Ajaccio,\n",
      "0.9792989492416382\n",
      "milieu rural,\n",
      "0.527215838432312\n",
      "À l'école,\n",
      "0.453364759683609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Ajaccio,',\n",
       " \"La famille Bonaparte vit à Ajaccio, rue Malerba (rue de la Mauvaise-Herbe, aujourd'hui rue Saint-Charles), dans une petite maison traditionnelle du XVIIIe siècle, que Napoléon qualifiera lui-même de « misérable ». La Casa Buonaparte est habitée au rez-de-chaussée et au premier étage par les Bonaparte et au deuxième étage par leurs cousins, les Pozzo di Borgo. Ce voisinage est insupportable et les deux familles vivent dans une brouille continuelle. On raconte qu'un jour, une Pozzo di Borgo aurait jeté le contenu d'un pot de chambre par la fenêtre, sur Madame Letizia.\",\n",
       " 'https://fr.wikipedia.org/wiki/Napol%C3%A9on_Ier')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askerfr.run(\"Où est né Napoléon Bonaparte ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mickey']\n",
      "subject found by ent :  ORG Mickey \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Oswald le lapin chanceux,',\n",
       " \"Mickey a été créé en 1928, après que Walt Disney eut dû laisser son premier personnage créé avec Ub Iwerks, Oswald le lapin chanceux, à son producteur. Les premiers courts métrages qui le mettent en scène sont principalement animés par Ub Iwerks, associé de Walt Disney au sein des studios Disney (alors installés dans le studio d'Hyperion Avenue). Il devient plus tard aussi un personnage de bandes dessinées, de longs métrages, de séries télévisées et d'une myriade de produits dérivés.\",\n",
       " 'https://fr.wikipedia.org/wiki/Mickey_Mouse')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askerfr.run(\"Qui est Mickey ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"IndexError: index out of range in self\"\n",
    "#\"CamemBERT, like RoBERTa, does not make use of token type IDs.\"\n",
    "\n",
    "#askerfr.generateAnswer(\"Où est né Napoléon Bonaparte ?\",\"Napoléon Bonaparte, né le 15 août 1769 à Ajaccio et mort le 5 mai 1821 sur l'île Sainte-Hélène, est un militaire et homme d'État français, premier empereur des Français, du 18 mai 1804 au 6 avril 1814 et du 20 mars au 22 juin 1815, sous le nom de Napoléon Ier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of pipeline with very specified context (work very)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5439729690551758"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlpp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')\n",
    "\n",
    "answers = []\n",
    "scores = []\n",
    "\n",
    "answer = nlpp({\n",
    "    'question': \"Qui est Claude Monet?\",\n",
    "    'context': \"Claude Monet, né le 14 novembre 1840 à Paris et mort le 5 décembre 1926 à Giverny, est un peintre français et l’un des fondateurs de l'impressionnisme.\"\n",
    "})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7686043977737427,\n",
       " 'start': 49,\n",
       " 'end': 67,\n",
       " 'answer': 'entre 2030 et 2052'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlpp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')\n",
    "\n",
    "answers = []\n",
    "scores = []\n",
    "\n",
    "answer = nlpp({\n",
    "    'question': \"Quand risquons nous d'atteindre un réchauffement à 1.5 degrés?\",\n",
    "    'context': \"Le réchauffement planétaire atteindra les 1,5 °C entre 2030 et 2052 si la température continue d'augmenter à ce rythme. Le RS15 (rapport spécial sur le réchauffement climatique de 1,5 °C) résume, d'une part, les recherches existantes sur l'impact qu'un réchauffement de 1,5 °C aurait sur la planète et, d'autre part, les mesures nécessaires pour limiter ce réchauffement planétaire.\"\n",
    "})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.002875491976737976,\n",
       " 'start': 238,\n",
       " 'end': 298,\n",
       " 'answer': \"l'impact qu'un réchauffement de 1,5 °C aurait sur la planète\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlpp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')\n",
    "\n",
    "answers = []\n",
    "scores = []\n",
    "\n",
    "answer = nlpp({\n",
    "    'question': \"Quelles recherches sont résumées dans ce rapport ?\",\n",
    "    'context': \"Le réchauffement planétaire atteindra les 1,5 °C entre 2030 et 2052 si la température continue d'augmenter à ce rythme. Le RS15 (rapport spécial sur le réchauffement climatique de 1,5 °C) résume, d'une part, les recherches existantes sur l'impact qu'un réchauffement de 1,5 °C aurait sur la planète et, d'autre part, les mesures nécessaires pour limiter ce réchauffement planétaire.\"\n",
    "})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import pipeline\n",
    "#\n",
    "#nlp = pipeline('question-answering', model='camembert/camembert-large', tokenizer='camembert/camembert-large')\n",
    "#\n",
    "#answers = []\n",
    "#scores = []\n",
    "#\n",
    "#answer = nlp({\n",
    "#    'question': \"Qui est Claude Monet?\",\n",
    "#    'context': \"Claude Monet, né le 14 novembre 1840 à Paris et mort le 5 décembre 1926 à Giverny, est un peintre français et l’un des fondateurs de l'impressionnisme.\"\n",
    "#})\n",
    "#\n",
    "#answer\n",
    "\n",
    "#CAMEMBERT LARGE is not expected to do question answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes : Extraction ent for wikipedia with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heures Creuses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Heure creuse',\n",
       " 'Gare de Neuilly-Plaisance',\n",
       " 'Chauffe-eau',\n",
       " 'Ligne P du Transilien',\n",
       " 'Ligne L du Transilien',\n",
       " 'Gare de Bry-sur-Marne (RATP)',\n",
       " 'Tesla Powerwall',\n",
       " 'Ligne J du Transilien',\n",
       " \"Ligne A du RER d'Île-de-France\",\n",
       " 'Ligne H du Transilien']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp2 = spacy.load('fr_core_news_md')\n",
    "\n",
    "question = \"Où sont détaillées les Heures Creuses ?\"\n",
    "\n",
    "query = \" \".join([str(x) for x in nlp2(question).ents])\n",
    "query = query if len(query) > 0 else question\n",
    "\n",
    "print(query)\n",
    "\n",
    "relevant_title = wikipedia.search(query,results=10)\n",
    "\n",
    "relevant_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection with langdetect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(\"War doesn't show who's right, just who's left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Ein, zwei, drei, vier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Où sont détaillées les Heures Creuses ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
