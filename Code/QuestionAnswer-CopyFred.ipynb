{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "import truecase\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ROOT\n",
      "what det\n",
      "direction dobj\n",
      "does aux\n",
      "the det\n",
      "sun nsubj\n",
      "rise pcomp\n",
      "in prep\n",
      "the det\n",
      "morning pobj\n",
      "? punct\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp('From what direction does the sun rise in the morning ?')\n",
    "for token in doc : \n",
    "    print(token.text, token.dep_)\n",
    "#for token in doc: \n",
    "    #print(token.text)\n",
    "    #print(token.dep_)\n",
    "    #print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Model function\n",
    "Use loaded BERT model to return answer<br>\n",
    "**Input** : Question, Context text <br>\n",
    "**return** : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    print(\"I'm looking for an answer, please wait ...\")\n",
    "    # == Tokenize ==\n",
    "    # use a python dictonary so run on CPU\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    print(\"-Tokenization\")\n",
    "    input_ids = tokenizer.encode(question, answer_text, add_special_tokens=True)\n",
    "    #print(\"input ids : \", input_ids)\n",
    "    #print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens including the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    # by default on CPU, use model.to(device) to select GPU !?\n",
    "    print(\"-Forward pass on the model\")\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from \n",
    "\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    #print(type(start_scores))\n",
    "    #print(start_scores.size())\n",
    "    #print(start_scores[0,answer_start])\n",
    "    #print(end_scores[0,answer_end])\n",
    "    \n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Question Processing\n",
    "Extract subjet _(and more?)_ from the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ancient greek]\n",
      "['nsubj']\n",
      "subject found by noun: ancient greek \n",
      "\n",
      "['Barack Obama']\n",
      "['nsubj']\n",
      "subject found by ent :  PERSON Barack Obama \n",
      "\n",
      "['ROMEO AND JULIET']\n",
      "['attr', 'nsubj', 'pobj', 'nsubj', 'dobj', 'conj']\n",
      "subject found by ent :  WORK_OF_ART ROMEO AND JULIET \n",
      "\n",
      "[the size, the moon]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: the moon \n",
      "\n",
      "['The Lord of The Rings']\n",
      "['nsubj', 'dobj', 'pobj']\n",
      "subject found by ent :  WORK_OF_ART The Lord of The Rings \n",
      "\n",
      "[]\n",
      "['attr']\n",
      "subject not found, please try another formulation \n",
      "\n",
      "[the sky]\n",
      "['attr', 'nsubj']\n",
      "subject found by noun: the sky \n",
      "\n",
      "[the color, the sky]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: the sky \n",
      "\n",
      "[the meaning, bread]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: bread \n",
      "\n",
      "[the meaning]\n",
      "['attr', 'nsubj']\n",
      "subject found by noun: the meaning \n",
      "\n",
      "[the meaning, OMG]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: OMG \n",
      "\n",
      "[the meaning]\n",
      "['attr', 'nsubj']\n",
      "subject found by noun: the meaning \n",
      "\n",
      "[the third color, the french flag]\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by noun: the french flag \n",
      "\n",
      "['Nirvana']\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by ent :  PERSON Nirvana \n",
      "\n",
      "[hops, the brewing process]\n",
      "['nsubjpass', 'pobj']\n",
      "subject found by noun: the brewing process \n",
      "\n",
      "[what direction, the sun, the morning]\n",
      "['dobj', 'nsubj', 'pobj']\n",
      "subject found by noun: the morning \n",
      "\n",
      "['Nirvana']\n",
      "['attr', 'nsubj', 'pobj']\n",
      "subject found by ent :  PERSON Nirvana \n",
      "\n",
      "[est, Jeanne Hachete]\n",
      "['nsubj', 'dobj']\n",
      "subject found by noun: Jeanne Hachete \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject': 'Jeanne Hachete', 'infos': [est]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_subject_with_spacy(question):\n",
    "    \n",
    "    #question = truecase.get_true_case(question) #le truecaser est un peu bidon j'ai l'impression\n",
    "    #print(question)\n",
    "    \n",
    "    subject_dict = {'subject' : '', 'infos' : []} #dictionnaire qui contiendra le sujet, et les infos complémentaires\n",
    "    \n",
    "    osef_list = ['who','why','what','when','which','how', 'Who','Why','What','When','Which', 'How'] #noun to not take into account\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    #on prépare une liste des noms communs (ou plus précisent chunks, qui peuvent être des groupes nominaux plus larges, des unités de sens) de la question\n",
    "    \n",
    "    nouns_list = []\n",
    "    dep_list = []\n",
    "    for noun in doc.noun_chunks :\n",
    "        dep_list.append(noun.root.dep_)\n",
    "        nouns_list.append(noun)\n",
    "           \n",
    "    #on enlève de la liste des potentiels sujets les mots interrogatifs venant de osef_list\n",
    "    for noun in nouns_list : \n",
    "        if str(noun) in osef_list : \n",
    "            nouns_list.remove(noun)\n",
    "                \n",
    "    #on crée une liste d'entité nommées de la phrase. S'il y en a une dans la question, alors c'est le sujet\n",
    "    #nn crée également une liste qui va contenir les labels de ces entités nommées, car certains types d'entités ne nous intéressent pas\n",
    "    #les labels qui nous intéressent sont dans la liste relevant_labels\n",
    "    \n",
    "    #si il y a des entités nommées, on les utilise comme sujet et les chunks alentours comme infos supplémentaires\n",
    "    #si il n'y a pas d'entités nommées, on va uniquement regarder les chunks (dans le 'else')\n",
    "    \n",
    "    ents_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    relevant_labels = ['PERSON','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW']\n",
    "    for ent in doc.ents :\n",
    "        if ent.label_ in relevant_labels : \n",
    "            ents_list.append(ent.text)\n",
    "            labels_list.append(ent.label_)\n",
    "            #dep_list.append(ent.dep_) #pour tests sur depencies\n",
    "            \n",
    "    if ents_list and labels_list : \n",
    "        print(ents_list)\n",
    "        print(dep_list)\n",
    "        print('subject found by ent : ', labels_list[-1] , ents_list[-1], '\\n')\n",
    "        subject_dict['subject'] = ents_list[-1] #on renvoie la dernière entité nommée pertinente trouvée\n",
    "        for other_noun in nouns_list : \n",
    "            subject_dict['infos'].append(other_noun)\n",
    "        return(subject_dict)\n",
    "            \n",
    "    \n",
    "    else : \n",
    "            \n",
    "    #si notre liste de chunks potentiels sujets est vide : pas de sujet\n",
    "    #si elle est égal à 1 : pas de doute, le sujet est cet élément\n",
    "    #si elle est plus grande que 1, le sujet est le deuxième élément\n",
    "    #règle simpliste mais qui semble suivre la logique de la formulation d'une question : c'est souvent le second nom qui est le sujet dans les questions qui en comportent deux, j'ai l'impression \n",
    "        \n",
    "        print(nouns_list)\n",
    "        print(dep_list)\n",
    "        if(len(nouns_list)) == 0 :\n",
    "            print(\"subject not found, please try another formulation\", '\\n')\n",
    "        else :\n",
    "            print(\"subject found by noun: \" + str(nouns_list[-1]), '\\n')\n",
    "            subject_dict['subject'] = str(nouns_list[-1]) #le sujet est le dernier chunk\n",
    "            for other_noun in nouns_list[0:-1] : #dans ces cas de figure avec + d'un nom, il faudra quand même récupérer le nom qui n'est pas le sujet, pour aller l'utiliser en scrappant la page wiki du sujet\n",
    "                subject_dict['infos'].append(other_noun)\n",
    "            return(subject_dict)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "#test\n",
    "\n",
    "#doc = nlp(\"Who wrote The Lords of The Rings\")\n",
    "#for ents in doc.ents : \n",
    "    #print(ents.text, ents.label_)\n",
    "\n",
    "\n",
    "extract_subject_with_spacy('When was ancient greek used ?')\n",
    "extract_subject_with_spacy('When is Barack Obama born ?')\n",
    "extract_subject_with_spacy('WHAT IS THE LAST NAME OF THE AUTHOR WHO WROTE “ROMEO AND JULIET”?')\n",
    "extract_subject_with_spacy(\"What is the size of the moon?\")\n",
    "extract_subject_with_spacy(\"Who wrote The Lord of The Rings?\")\n",
    "extract_subject_with_spacy('What is the ?')\n",
    "extract_subject_with_spacy('What is the sky?')\n",
    "extract_subject_with_spacy('What is the color of the sky ?')\n",
    "#\n",
    "extract_subject_with_spacy('What is the meaning of \"bread\" ?') #pas de pb\n",
    "extract_subject_with_spacy('What is the meaning of \"omg\" ?') #les sigles ne sont pas reconnus\n",
    "extract_subject_with_spacy('What is the meaning of \"OMG\" ?') #ah bah en majuscule si\n",
    "extract_subject_with_spacy('What is the meaning of \"why\" ?') #cas (très) rare et spécial à gérer, quand une question porte sur un mot interrogatif qui est dans osef_list (faudra mettre une condition genre si y'a + d'un mot interrogatif, ne supprimer que le premier de noun_list)\n",
    "extract_subject_with_spacy('What is the third color of the french flag ?') #propre\n",
    "extract_subject_with_spacy(\"What is the color of Nirvana's second album ?\") #stylé que ça trouve le sujet dans ce genre de cas. Une future grosse tâche : trouver les articles wikipédia à partir de ce genre de paraphrase\n",
    "extract_subject_with_spacy('When are hops added in the brewing process ?')\n",
    "extract_subject_with_spacy('From what direction does the sun rise in the morning ?')\n",
    "extract_subject_with_spacy(\"What is the height of Nirvana's singer ?\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Wikipedia API\n",
    "Try to found the most relevant context text to give as BERT input. <br>\n",
    "Get a wikipedia article, and scrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == WIP ==\n",
    "# découpé en paragraphe (le model a une limite de 512 token pour le text en entré)\n",
    "def get_wiki_and_split(subject):\n",
    "    text = wikipedia.summary(subject)\n",
    "    print(len(text))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia.search(\"salsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = wikipedia.summary('salsa')\n",
    "#list = text.split('.')\n",
    "#print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20bae9ed8e6446290be75d09f2ee32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(height='0px', margin='100px 0 0 100px', width='400px')), Button(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35cba53416c4db9893353c4b213a05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='auto', margin='50px 0 100px 100px', width='450px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "#Widgets layout difinition\n",
    "layout = widgets.Layout(width='400px', height='0px', margin='100px 0 0 100px')\n",
    "bLayout = widgets.Layout(width='50px', height='28px', margin='100px 0 0 0px')\n",
    "outLayoutPropre = widgets.Layout(width='480px', height='100px', margin='50px 0 100px 100px')\n",
    "outLayoutTest = widgets.Layout(width='450px', height='auto', margin='50px 0 100px 100px')\n",
    "#titleLayout = widgets.Layout(width='450px', height='auto', margin='0px 0 0px 100px')\n",
    " \n",
    "#Widgets object definition\n",
    "text = widgets.Text(layout=layout)\n",
    "button = widgets.Button(description = 'Ask', layout = bLayout)\n",
    "out = widgets.Output(layout=outLayoutTest)#layout=outLayout\n",
    "#out = widgets.HTML(layout = outLayout, value= '<style>.text {width: 480px; heigh: 100px;}</style> <p class=\"text\">'+ out_value +' </p>')\n",
    " \n",
    "def button_on_click(self):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        subject = extract_subject_with_spacy(question=text.value)['subject']\n",
    "        if subject is not None:\n",
    "            context = wikipedia.summary(subject)\n",
    "            \n",
    "            answer = generate_answer(text.value, context[:2000])\n",
    "            #out.clear_output()\n",
    "            print(\"Here is what i found: \\n\"+ answer)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "button.on_click(button_on_click)\n",
    "\n",
    "display(widgets.HBox((text, button,)))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
