{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Model function\n",
    "Use loaded BERT model to return answer<br>\n",
    "**Input** : Question, Context text <br>\n",
    "**return** : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    print(\"I'm looking for an aswer, wait please ...\")\n",
    "    # == Tokenize ==\n",
    "    # use a python dictonary so run on CPU\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    print(\"-Tokenization\")\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    #print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    # by default on CPU, use model.to(device) to select GPU !?\n",
    "    print(\"-Forward pass on the model\")\n",
    "    \n",
    "    #move tensor to device\n",
    "    input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "    segment_ids_tensor = torch.tensor([segment_ids]).to(device)\n",
    "    \n",
    "    start_scores, end_scores = model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                 token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from \n",
    "    # ici retourne un score pour chaque token\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    # RÃ©cupÃ©re le socre\n",
    "    start_score = float(start_scores[0,answer_start])\n",
    "    end_score = float(end_scores[0,answer_end])\n",
    "    \n",
    "    \n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer, start_score+end_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm looking for an aswer, wait please ...\n",
      "-Tokenization\n",
      "-Forward pass on the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('initial public offering', 14.472150325775146)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"What is IPO?\", \"Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering (IPO) took place on August 19, 2004, and Google moved to its headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google is Alphabet's leading subsidiary and will continue to be the umbrella company for Alphabet's Internet interests. Sundar Pichai was appointed CEO of Google, replacing Larry Page who became the CEO of Alphabet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2123, 1005, 1056, 2113, 102, 1057, 1012, 1047, 3475, 1005, 1056, 1037, 2518, 102]\n",
      "['[CLS]', 'i', 'don', \"'\", 't', 'know', '[SEP]', 'u', '.', 'k', 'isn', \"'\", 't', 'a', 'thing', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"i don't know\", \"U.K isn't a thing\")\n",
    "print(input_ids)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Question Processing\n",
    "Extract subjet _(and more?)_ from the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subj(question):\n",
    "    subject = None\n",
    "    token = nltk.word_tokenize(question)\n",
    "    #print(token)\n",
    "    pos_token = nltk.pos_tag(token)\n",
    "    for item in pos_token:\n",
    "        if item[1] == 'NN':\n",
    "            subject = item[0]\n",
    "    if subject is not None:\n",
    "        print(\"Subject found: \" + subject)\n",
    "    else:\n",
    "        print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "    return  subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'truecase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d00334485f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtruecase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'truecase'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import truecase\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_subject(question):\n",
    "    #noun to not take in count\n",
    "    subj_list = []\n",
    "    osef_list = ['who','why','what','when','which','how']\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    ents = doc.ents\n",
    "    nouns = doc.noun_chunks\n",
    "    # == Check for Named Entity ==\n",
    "    if len(ents) != 0:\n",
    "        for ent in doc.ents:\n",
    "            subj_list.append(ent.text)\n",
    "            print(\"Named entity: \" + ent.text) #, ent.start_char, ent.end_char, ent.label_\n",
    "    # == Check for Noun Chunk ==\n",
    "    elif len(list(nouns)) != 0:\n",
    "        for item in nouns:\n",
    "            if str(item) not in osef_list:\n",
    "                subj_list.append(str(item))\n",
    "                print(\"Noun chunck: \" + str(item))\n",
    "                \n",
    "    #print(\"Subject found: \" + str(item))\n",
    "    #return str(item)\n",
    "    #print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "    if len(subj_list) != 0:\n",
    "        print(\"Subject selected: \" + subj_list[-1])\n",
    "        return subj_list[-1]\n",
    "    else:\n",
    "        print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "        return None\n",
    "#test\n",
    "#extract_subject_with_spacy('which is the most common use of opt-in e-mail marketing ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how the dog dies?\"\n",
    "\n",
    "#question = truecase.get_true_case(question)\n",
    "print(question)\n",
    "\n",
    "extract_subject(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"In\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "Try to found the most relevant context text to give as BERT input. <br>\n",
    "Get a wikipedia article, and scrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia #use to search article name\n",
    "import wikipediaapi #use to acess article page and content\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Firt', 'senteces.']\n",
      "[\"I'dont\", 'if', 'U.K', 'is', 'in', 'Europe.']\n",
      "['But', 'osef', '.Lol']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Firt senteces. I'dont if U.K is in Europe. But osef .Lol\"\n",
    "doc = nlp(text)\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(sent.text)\n",
    "for s in sentences:\n",
    "    print(s.split(\" \"))\n",
    "#word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lil Wayne\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Lil_Wayne'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_retrivial = wikipedia.search('lil wayne')\n",
    "print(wiki_retrivial[0])\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        #extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "        )\n",
    "\n",
    "page = wiki.page(wiki_retrivial[0])\n",
    "print(page.exists())\n",
    "page.title\n",
    "#page.text\n",
    "#page.summary\n",
    "page.fullurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*: Computing - 150\n",
      "*: People - 286\n",
      "*: Roller coasters - 177\n",
      "*: Vehicles - 105\n",
      "*: Weaponry - 167\n",
      "*: Other uses - 266\n",
      "*: See also - 84\n"
     ]
    }
   ],
   "source": [
    "def get_sections(sections, level=0):\n",
    "        for s in sections:\n",
    "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, len(s.text)))\n",
    "                get_sections(s.sections, level + 1)\n",
    "\n",
    "                \n",
    "\n",
    "def cut(text):\n",
    "    pass\n",
    "    \n",
    "\n",
    "page = wiki.page('python')  \n",
    "get_sections(page.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack Obama',\n",
       " 'Barack Obama Sr.',\n",
       " 'Family of Barack Obama',\n",
       " 'Presidency of Barack Obama',\n",
       " 'Barack Obama citizenship conspiracy theories',\n",
       " 'Electoral history of Barack Obama',\n",
       " 'Speeches of Barack Obama',\n",
       " 'Barack Obama 2008 presidential campaign',\n",
       " 'Early life and career of Barack Obama',\n",
       " 'List of federal judges appointed by Barack Obama']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wiki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-774c47d4685a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwiki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'donald trump'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wiki' is not defined"
     ]
    }
   ],
   "source": [
    "# == Loop == \n",
    "\n",
    "timer = time.perf_counter() \n",
    "\n",
    "page = wiki.page('donald trump')\n",
    "text = page.summary\n",
    "\n",
    "\n",
    "cuted = text.split(\"\\n\")\n",
    "\n",
    "answers = []\n",
    "scores = []\n",
    "timer = time.perf_counter() \n",
    "for p in cuted:\n",
    "    answer, score = generate_answer(\"when is Donal trump born ?\", p)\n",
    "    answers.append(answer)\n",
    "    scores.append(score)\n",
    "print(time.perf_counter() - timer)\n",
    "    \n",
    "max_value = max(scores)\n",
    "\n",
    "index = scores.index(max_value)\n",
    "print(max_value)\n",
    "print(answers[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = time.perf_counter() \n",
    "cuted = text.split(\"\\n\")\n",
    "print(time.perf_counter() - timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['june 14 , 1946', '1971', '2016', '2019', '2016', '2020']\n",
      "june 14 , 1946\n"
     ]
    }
   ],
   "source": [
    "max_value = max(scores)\n",
    "\n",
    "index = scores.index(max_value)\n",
    "print(index)\n",
    "\n",
    "print(answers)\n",
    "\n",
    "print(answers[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3661\n",
      "I'm looking for an aswer, wait please ...\n",
      "-Tokenization\n",
      "-Forward pass on the model\n",
      "I'm looking for an aswer, wait please ...\n",
      "-Tokenization\n",
      "-Forward pass on the model\n",
      "14.267158899999686\n",
      "june 14 , 1946  ||  8.51047420501709\n",
      "[SEP]  ||  2.7942256927490234\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "timer = time.perf_counter() \n",
    "a1,s1 = generate_answer(\"when is Donal trump born ?\", text[:2000])\n",
    "a2,s2 = generate_answer(\"when is Donal trump born ?\", text[2000:])\n",
    "print(time.perf_counter() - timer)\n",
    "\n",
    "print(a1,\" || \", s1)\n",
    "print(a2,\" || \", s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Firt', 'senteces.']\n",
      "[\"I'dont\", 'if', 'U.K', 'is', 'in', 'Europe.']\n",
      "['But', 'osef', '.Lol']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Lil_Wayne'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c   e ci est un te.st!\n",
      "c   e ci est un te st!\n",
      "c   e ci est un te st!\n",
      "c   e ci est un te st!\n",
      "c   e ci est un te st!\n",
      "c   e ci est un te st \n",
      "c   e ci est un te st \n",
      "['c', '', '', 'e', 'ci', 'est', 'un', 'te', 'st', '']\n",
      "*: Computing - 150\n",
      "*: People - 286\n",
      "*: Roller coasters - 177\n",
      "*: Vehicles - 105\n",
      "*: Weaponry - 167\n",
      "*: Other uses - 266\n",
      "*: See also - 84\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Wars', 'Star Wars (film)', 'List of Star Wars films', 'Star Wars: The Rise of Skywalker', 'Star Wars Rebels', 'List of Star Wars characters', 'Star Wars: The Last Jedi', 'Star Wars: The Force Awakens', 'Star Wars: Squadrons', 'The Child (Star Wars)']\n"
     ]
    },
    {
     "ename": "PageError",
     "evalue": "Page id \"start wars\" does not match any pages. Try another id!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPageError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-47824d846fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#wikipedia.suggest(\"Lil Pump\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(wikipedia.suggest(\"OMG\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'missing'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPageError\u001b[0m: Page id \"start wars\" does not match any pages. Try another id!"
     ]
    }
   ],
   "source": [
    "subj = \"Star Wars\"\n",
    "search = wikipedia.search(subj)\n",
    "print(search)\n",
    "#wikipedia.suggest(\"Lil Pump\")\n",
    "#print(wikipedia.suggest(\"OMG\"))\n",
    "page = wikipedia.page(search[0])\n",
    "page.title\n",
    "#'\\n'\n",
    "#page.content\n",
    "#page.url\n",
    "#page.title\n",
    "#page.images[0]\n",
    "\n",
    "#try:\n",
    "#    page = wikipedia.page(\"OMG (disambiguation)\")\n",
    "#except wikipedia.exceptions.DisambiguationError as e:\n",
    "#    print(e.options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "PageError",
     "evalue": "Page id \"start wars\" does not match any pages. Try another id!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPageError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6b03a2f7b05f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'star wars'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#list = text.split('.')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(title, sentences, chars, auto_suggest, redirect)\u001b[0m\n\u001b[0;32m    229\u001b[0m   \u001b[1;31m# use auto_suggest and redirect to get the correct article\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;31m# also, use page's error checking to raise DisambiguationError if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[0mpage_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauto_suggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mpageid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\software_project\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'missing'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPageError\u001b[0m: Page id \"start wars\" does not match any pages. Try another id!"
     ]
    }
   ],
   "source": [
    "text = wikipedia.summary('star wars')\n",
    "#list = text.split('.')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8fd6ee4f1b4324a69b33f9bc2990ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(height='0px', margin='100px 0 0 100px', width='400px')), Button(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d755f9898aa64ecb91649ae2a59ae0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='auto', margin='50px 0 100px 100px', width='450px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "#Widgets layout difinition\n",
    "layout = widgets.Layout(width='400px', height='0px', margin='100px 0 0 100px')\n",
    "bLayout = widgets.Layout(width='50px', height='28px', margin='100px 0 0 0px')\n",
    "outLayoutPropre = widgets.Layout(width='480px', height='100px', margin='50px 0 100px 100px')\n",
    "outLayoutTest = widgets.Layout(width='450px', height='auto', margin='50px 0 100px 100px')\n",
    "#titleLayout = widgets.Layout(width='450px', height='auto', margin='0px 0 0px 100px')\n",
    " \n",
    "#Widgets object definition\n",
    "text = widgets.Text(layout=layout)\n",
    "button = widgets.Button(description = 'Ask', layout = bLayout)\n",
    "out = widgets.Output(layout=outLayoutTest)#layout=outLayout\n",
    "#out = widgets.HTML(layout = outLayout, value= '<style>.text {width: 480px; heigh: 100px;}</style> <p class=\"text\">'+ out_value +' </p>')\n",
    " \n",
    "def button_on_click(self):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "        subject = extract_subject(question=text.value)\n",
    "        \n",
    "        if subject is not None:\n",
    "            \n",
    "            context = wikipedia.summary(subject)\n",
    "            \n",
    "            answer, score = generate_answer(text.value, context[:2000])\n",
    "            #out.clear_output()\n",
    "            print()\n",
    "            print(\"Here what i found: \\n\"+ answer)\n",
    "            print()\n",
    "            print(\"Score: \" +str(score))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "button.on_click(button_on_click)\n",
    "\n",
    "display(widgets.HBox((text, button,)))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
