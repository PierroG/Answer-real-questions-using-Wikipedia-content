{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Model function\n",
    "Use loaded BERT model to return answer<br>\n",
    "**Input** : Question, Context text <br>\n",
    "**return** : Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    print(\"I'm looking for an aswer, wait please ...\")\n",
    "    # == Tokenize ==\n",
    "    # use a python dictonary so run on CPU\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    print(\"-Tokenization\")\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    #print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    # by default on CPU, use model.to(device) to select GPU !?\n",
    "    print(\"-Forward pass on the model\")\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from \n",
    "    # ici retourne un score pour chaque token\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    # RÃ©cupÃ©re le socre\n",
    "    start_score = float(start_scores[0,answer_start])\n",
    "    end_score = float(end_scores[0,answer_end])\n",
    "    \n",
    "    \n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer, start_score+end_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Question Processing\n",
    "Extract subjet _(and more?)_ from the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subject(question):\n",
    "    subject = None\n",
    "    token = nltk.word_tokenize(question)\n",
    "    #print(token)\n",
    "    pos_token = nltk.pos_tag(token)\n",
    "    for item in pos_token:\n",
    "        if item[1] == 'NN':\n",
    "            subject = item[0]\n",
    "    if subject is not None:\n",
    "        print(\"Subject found: \" + subject)\n",
    "    else:\n",
    "        print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "    return  subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opt-in e-mail marketing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_subject_with_spacy(question):\n",
    "    #noun to not take in count\n",
    "    subject_list = []\n",
    "    osef_list = ['who','why','what','when','which','how']\n",
    "    doc = nlp(question)\n",
    "    nouns = doc.noun_chunks\n",
    "    #print(len(list(nouns)))\n",
    "    #return nouns[-1]\n",
    "    \n",
    "    for item in nouns:\n",
    "        if str(item) not in osef_list:\n",
    "            subject_list.append(str(item))\n",
    "            #print(\"Subject found: \" + str(item))\n",
    "            #return str(item)\n",
    "    #print(\"Subject not found ðŸ˜”\\n Rephrase the question or try another one\")\n",
    "    return subject_list[-1]\n",
    "\n",
    "#test\n",
    "#extract_subject_with_spacy('which is the most common use of opt-in e-mail marketing ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load kaggle test file ===\n",
    "\n",
    "import json_lines\n",
    "\n",
    "file = 'C:/Users/pg540/Desktop/M2/Software_Project/data/simplified-nq-test.jsonl'\n",
    "data = []\n",
    "\n",
    "with open(file, 'rb') as f: # opening file in binary(rb) mode   \n",
    "    for item in json_lines.reader(f):\n",
    "        data.append(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in what release does the release track cardiac arrest come from ?\n",
      "['what release', 'the release track cardiac arrest']\n",
      "['prep', 'det', 'pcomp', 'aux', 'det', 'compound', 'compound', 'amod', 'nsubj', 'ROOT', 'prep', 'punct']\n"
     ]
    }
   ],
   "source": [
    "data[0].keys()\n",
    "doc = nlp('in what release does the release track cardiac arrest come from ?')\n",
    "print(doc)\n",
    "print([chunk.text for chunk in doc.noun_chunks])\n",
    "print([token.dep_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "Try to found the most relevant context text to give as BERT input. <br>\n",
    "Get a wikipedia article, and scrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == WIP ==\n",
    "# dÃ©coupÃ© en paragraphe (le model a une limite de 512 token pour le text en entrÃ©)\n",
    "def get_wiki_and_split(subject):\n",
    "    text = wikipedia.summary(subject)\n",
    "    print(len(text))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cardiac arrest'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search(\"the release track cardiac arrest\")\n",
    "#wikipedia.suggest(\"Lil Pump\")\n",
    "#print(wikipedia.suggest(\"OMG\"))\n",
    "page = wikipedia.page('the release track cardiac arrest')\n",
    "page.title\n",
    "#'\\n'\n",
    "#page.content\n",
    "#page.url\n",
    "#page.title\n",
    "#page.images[0]\n",
    "\n",
    "#try:\n",
    "#    page = wikipedia.page(\"OMG (disambiguation)\")\n",
    "#except wikipedia.exceptions.DisambiguationError as e:\n",
    "#    print(e.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikipedia.summary('salsa')\n",
    "list = text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9c2416d07c4f819930fd9cfb2d0659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(height='0px', margin='100px 0 0 100px', width='400px')), Button(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f715ed324b8c4cec977e310dedf251be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='auto', margin='50px 0 100px 100px', width='450px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "#Widgets layout difinition\n",
    "layout = widgets.Layout(width='400px', height='0px', margin='100px 0 0 100px')\n",
    "bLayout = widgets.Layout(width='50px', height='28px', margin='100px 0 0 0px')\n",
    "outLayoutPropre = widgets.Layout(width='480px', height='100px', margin='50px 0 100px 100px')\n",
    "outLayoutTest = widgets.Layout(width='450px', height='auto', margin='50px 0 100px 100px')\n",
    "#titleLayout = widgets.Layout(width='450px', height='auto', margin='0px 0 0px 100px')\n",
    " \n",
    "#Widgets object definition\n",
    "text = widgets.Text(layout=layout)\n",
    "button = widgets.Button(description = 'Ask', layout = bLayout)\n",
    "out = widgets.Output(layout=outLayoutTest)#layout=outLayout\n",
    "#out = widgets.HTML(layout = outLayout, value= '<style>.text {width: 480px; heigh: 100px;}</style> <p class=\"text\">'+ out_value +' </p>')\n",
    " \n",
    "def button_on_click(self):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "        subject = extract_subject_with_spacy(question=text.value)\n",
    "        \n",
    "        if subject is not None:\n",
    "            \n",
    "            context = wikipedia.summary(subject)\n",
    "            \n",
    "            answer, score = generate_answer(text.value, context[:2000])\n",
    "            #out.clear_output()\n",
    "            print()\n",
    "            print(\"Here what i found: \\n\"+ answer)\n",
    "            print()\n",
    "            print(\"Score: \" +str(score))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "button.on_click(button_on_click)\n",
    "\n",
    "display(widgets.HBox((text, button,)))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
