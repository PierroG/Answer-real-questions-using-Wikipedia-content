{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "\n",
    "#utile pour le pos tagging\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# == not useful, Only for print token and id ==\n",
    "\n",
    "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "# tokenizer's behavior, let's also get the token strings and display them.\n",
    "\n",
    "def print_token_id(input_ids):\n",
    "    # For each token and its id...\n",
    "    for token, id in zip(tokens, input_ids):\n",
    "        # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "        if id == tokenizer.sep_token_id:\n",
    "            print('')\n",
    "        # Print the token string and its ID in two columns.\n",
    "        print('{:<12} {:>6,}'.format(token, id))\n",
    "        if id == tokenizer.sep_token_id:\n",
    "            print('')\n",
    "\n",
    "#tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, answer_text):\n",
    "    # == Tokenize ==\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # == Set Segment IDs ==\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # == Run Model ==\n",
    "    # Run our example through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from \n",
    "\n",
    "    \n",
    "    # donc on applique un argmax pour trouver le plus probable\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # == Print Answer without ## ==\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "    \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================================================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fonction qui extrait le dernier sujet de la question\n",
    "def extract_subject(question):\n",
    "    subject = None\n",
    "    token = nltk.word_tokenize(question)\n",
    "    print(token)\n",
    "    pos_token = nltk.pos_tag(token)\n",
    "    for item in pos_token:\n",
    "        print(item)\n",
    "        if item[1] == 'NN' or item[1] == 'NNP':\n",
    "            subject = item[0]\n",
    "    return  subject\n",
    "\n",
    "# == WIP ==\n",
    "# découpé en paragraphe (le model a une limite de 512 token pour le text en entré)\n",
    "def get_wiki_and_split(subject):\n",
    "    answer_text = wikipedia.summary(subject)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'is', 'Barack', 'Obama', '?']\n",
      "('Who', 'WP')\n",
      "('is', 'VBZ')\n",
      "('Barack', 'NNP')\n",
      "('Obama', 'NNP')\n",
      "('?', '.')\n",
      "The Subject is : Obama\n",
      "\n",
      "The input has a total of 417 tokens.\n",
      "Answer: \"barack hussein obama ii\"\n",
      "Barack Hussein Obama II ( (listen) bə-RAHK hoo-SAYN oh-BAH-mə; born August 4, 1961) is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, Obama was the first African-American president of the United States. He previously served as a U.S. senator from Illinois from 2005 to 2008 and an Illinois state senator from 1997 to 2004.\n",
      "Obama was born in Honolulu, Hawaii. After graduating from Columbia University in \n"
     ]
    }
   ],
   "source": [
    "#Enter HERE the question you want :\n",
    "question = \"Who is Barack Obama ?\"\n",
    "\n",
    "subject = extract_subject(question)\n",
    "\n",
    "print(\"The Subject is : \"+subject)\n",
    "print(\"\")\n",
    "\n",
    "if subject is not None:\n",
    "    answer_text = wikipedia.summary(subject)\n",
    "    generate_answer(question, answer_text[:2000])\n",
    "    print(answer_text[:500])\n",
    "else:\n",
    "    print(\"Subject not found :/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
